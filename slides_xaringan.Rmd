---
title: "Estadística Bayesiana y Programación Probabilística"
subtitle: "O cómo dejé de preocuparme y aprendí a amar la incertidumbre"
author: "Adolfo Martínez"
date: "2017/03/28"
output:
  xaringan::moon_reader:
    css: ["default", "portada.css"]
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

## Reducir Incertidumbre

Históricamente:

- Oráculos
- Religión
- Empirismo
- Método Científico
- Análisis de Datos

---
class: middle, center, inverse
## Estadística Frecuentista

---
## Estadística Frecuentista

Históricamente desarrollada a partir del estudio de **frecuencias**. Toma éstas como medida objetiva de la realidad y aproximación de la **probabilidad** "real"

- Prueba (contraste) de hipótesis
- Diseño de experimentos
- Predicción

---
background-image: url(resources/uniqueevent.png)
background-size: 500px
background-position: 50% 70%

## Estadística Frecuentista 
### Algunos problemas


Probabilidad de eventos únicos y cantidades fijas pero desconocidas

---
background-image: url(resources/test.gif)
background-size: 350px
background-position: 50% 70%

## Estadística Frecuentista 
### Algunos problemas

Técnicas con fundamento teórico débil (*e.g.* valores p)

* Juzga $H$ basado en $P(D|H)$

---
class: middle, center, inverse
## Machine Learning

---
## Machine Learning

Estudio y construcción de algoritmos que **aprenden** y realizan **predicciones** basados en datos

- Predicción
- Clustering
- Minado de Reglas

---
## Machine Learning 
### Algunos problemas

Responden una pregunta específica

* Por ejemplo, estimar una respuesta $y$ dados los datos $X$, *i.e.* $y = \hat{f}(x)$
* Bajo pérdida cuadrática: $\hat{f}(x) = E[Y | X = x]$
* $P(Y | X = x) > \alpha$
* En muchos algoritmos, no hay una respuesta inmediata a esta pregunta 

---
background-image: url(resources/mlp.png)
background-size: 500px
background-position: 50% 70%

## Machine Learning 
### Algunos problemas

Dificultad en la interpretación

---
background-image: url(resources/notagorilla.png)
background-size: 400px
background-position: 50% 70%
## Machine Learning 
### Algunos problemas

Sobrecertidumbre

---
class: middle, center, inverse
## Estadística Bayesiana

---
## Estadística Bayesiana

Incorpora la **incertidumbre** subjetiva o **creencias inciales** como información previa a la inferencia. La **probabilidad** es una medida de incertidumbre, no una frecuencia

- Prueba (contraste) de hipótesis
- Diseño de experimentos
- Predicción

---
## Estadística Bayesiana 
### vs. Frecuentismo

Tiene una manera clara y bien fundamentada de asignar probabilidad a eventos únicos o cantidades desconocidas

<center> $$P(\theta|D) \propto P(D|\theta)P(\theta)$$ </center>

* $P(\theta)$ se obtiene de las creencias iniciales (*a priori*)
* De no existir, se pueden usar *a priori* no informativas

---
background-image: url(resources/posterior.png)
background-size: 500px
background-position: 50% 75%
## Estadística Bayesiana 
### vs. Frecuentismo

Tiene una manera clara y bien fundamentada de calcular la probabilidad de una hipótesis

* Juzga $H$ basado en $P(H|D)$


---
## Estadística Bayesiana 
### vs. Machine Learning

Puede resolver una gran cantidad de preguntas acerca de la variable de respuesta

```{r predictiva, echo=F, message=F, warning=F, fig.height=3, fig.align='center'}
library(ggplot2)
library(dplyr)
d <- data.frame(x=rnorm(1000)+rexp(1000))
area <- data.frame(x = density(d$x)$x, y = density(d$x)$y) %>% filter(0 < x, x < 2.5)
ggplot(d, aes(x=x)) + geom_density() + geom_ribbon(data=area, aes(x=x, ymax=y), ymin=0) + theme_minimal() + 
  xlab(expression(paste(tilde(y), "|", tilde(x), "," , theta, ",", D))) +
  ylab(expression(paste("P(", tilde(y), "|", tilde(x), "," , theta, ",", D, ")")))
```

El área sombreada representa $P(0 < \tilde{y} < 2.5 | \tilde{x}, X, \theta)$

---
## Estadística Bayesiana 
### vs. Machine Learning

Como la Frecuentista, los modelos básicos son fácilmente interpretables

- Por ejemplo, en un modelo lineal bayesiano, las posteriores de los coeficientes $\beta_i$, describen la incertidumbre acerca de su valor

```{r posteriores, echo=F, warning=F, message=F, fig.height=4, fig.align='center'}
library(tidyr)
library(ggplot2)
n <- 10000
data.frame(beta0 = rnorm(n, m=10, s=2), beta1 = rnorm(n, m=5, s=1.5), beta2 = rnorm(n), 
           beta3 = rnorm(n, m=-5, s=0.1)) %>%
  gather %>%
  mutate(key = factor(key, labels=c("beta[0]", "beta[1]", "beta[2]", "beta[3]"))) %>%
  ggplot(aes(x=value)) + geom_density() + theme_minimal() + scale_y_continuous(labels = NULL) + 
  facet_wrap(~key, ncol=2, scales="free_y", labeller=label_parsed) + 
  geom_vline(xintercept = 0, linetype = "dotted") +
  xlab(expression(paste(beta[i], "|", D))) + ylab(expression(paste("P(", beta[i], "|", D, ")")))
```

---
## Estadística Bayesiana 
### vs. Machine Learning

La incertidumbre se conserva en cada paso de la inferencia

* Por ejemplo, terminada la inferencia podemos calcular la probabilidad de que los parámetros sean cercanos a $0$
* O bien, además del estimado puntual, calcular un intervalo de **probabilidad** para $Y$
* Conservar y conocer esta incertidumbre nos permite tomar mejores decisiones al predecir (por ejemplo, elegir no hacerlo)

---
## Estadística Bayesiana 
### Algunos problemas

### Cálculo de la posterior - Dificultad Matemática

* Para calcular la posterior de manera exacta, necesitamos resolver: $P(D) = \int_{\theta}P(D|\theta)P(\theta)d\theta$
* La distribución predictiva se obtiene a través de una integral similar: $P(\tilde{y} | \tilde{x}, \theta, D) = \int_{\theta}P(\tilde{y}|\tilde{x},\theta)P(\theta|D)d\theta$
* Estos problemas no siempre tienen una solución analítica

---
## Estadística Bayesiana 
### Algunos problemas

### Cálculo de la posterior - Dificultad Computacional

* Las integrales mencionadas anteriormente se pueden calcular de manera numérica
* Los métodos más populares para esto son los de **Markov Chain Monte Carlo (MCMC)**
* Aunque estos métodos típicamente requieren ajuste para lograr una convergencia rápida, avances recientes hacen esto innecesario.

---
class: middle, center, inverse
## Programación Probabilística

---
## Programación Probabilística

Una manera declarativa y sencilla de definir modelos jerárquicos Bayesianos, sin necesidad de resolver el problema matemático específico para cada modelo

```{python, echo=T, eval=F}
import pymc3 as pm

with pm.Model() as model:
    # hyper-parameters
    alpha = 1/data.mean()
    
    # parameters
    tau = pm.DiscreteUniform("tau", lower=0, upper=n_obs)
    lambda1 = pm.Exponential("lambda1", alpha)
    lambda2 = pm.Exponential("lambda2", alpha)
    lambda_i = pm.math.switch(tau >= idx, lambda1, lambda2)
    
    # observations
    obs = pm.Poisson("obs", lambda_i, observed=data)
```

---
background-image: url(resources/advi.png)
background-size: 600px
background-position: 50% 90%
## Programación Probabilística 
### Inferencia

Incluye los métodos computacionales más avanzados para el paso inferencial
```{python, echo=T, eval=F}
with model:
    # Do Inference
    step = pm.advi()
    trace = pm.sample(num_samples, step = step)
```

---
## Programación Probabilística 
### PyMC3
```{python, echo=T, eval=F}
model = pm.Model()
with model:
    # Create Model
    p_coef = Cauchy(0, 2.5)
    repaid = Bernoulli(logit(X * p_coef),
                       observed=logit(repaid_actual))

    # Do Inference
    start = pm.find_MAP()
    step = pm.NUTS()
    trace = pm.sample(num_samples, step, start,
                      progressbar=True)
```

---
## Programación Probabilística 
### Stan
```{stan output.var="lol" , echo=T, eval=F}
data {
    int<lower=0> N;
    int<lower=0> N_features;
    matrix[N, N_features] X;
    int<lower=0,upper=1> repaid[N];
}
parameters {
    vector[N_features] p_coef;
}
model {
    vector[N] p;
    p_coef ~ cauchy(0, 2.5);
    p = logit(X * p_coef);
    repaid ~ bernoulli(p);
}
```

---
## Programación Probabilística 
### Anglican
```{python, echo=T, eval=F}
;; Create Model
(defquery gaussian-model [data]
  (let [mu (sample (normal 1 (sqrt 5)))
        sigma (sqrt 2)]
    (doall (map (fn [x] (observe (normal mu sigma) x)) data))
    mu))
  
;; Do Inference  
(def posterior 
  ((conditional gaussian-model :smc :number-of-particles 10) dataset))
  
(def posterior-samples (repeatedly 20000 #(sample* posterior)))
```

---
## ¿Cómo Modelar?
- Tener una o varias preguntas
- Pensar cómo se pudieron haber **generado** los datos
- Escoger **distribuciones** que representen dichos datos
- Modelar **relaciones** entre variables (*e.g.* linealmente, árbol de decisión)
- Modelar parámetros con distribuciones que representen las **creencias iniciales** adecuadamente

---
class: middle, center, inverse
## Por ejemplo...

---
## Ejemplo: Cambio de Media
- *Data*: Número de mensajes de texto recibidos cada día
- ¿Existe un cambio súbito en esta variable?
```{r, echo=F, warning=F, message=F, fig.height=6, fig.align='center', fig.width=12, fig.align='center'}
counts <- read.csv("data/txtdata.csv", header=F)
names(counts) <- c("n")
counts$t <- 1:nrow(counts)

ggplot(counts, aes(x=t, y=n)) + geom_bar(stat="identity") + theme_minimal() + xlab("Día") + ylab("Mensajes recibidos")
```

---
## Ejemplo: Cambio de Media
- Supongamos que los mensajes se generan de manera aleatoria, según el día
- Una buena distribución para representar esto es la Poisson
```{r, echo=F, warning=F, message=F, fig.height=3.5, fig.align='center', fig.width=8}
plot.dist.disc <- function(df, title="") {
  ggplot(df, aes(x=x, y=y)) + geom_bar(stat="identity") + 
    theme_minimal() + xlab("x") + ylab("P(X=x)") + 
    ggtitle(title)
}
plot.dist.cont <- function(df, title="") {
  ggplot(df, aes(x=x, y=y)) + geom_line() +
    theme_minimal() + xlab("x") + ylab("P(X=x)") + 
    ggtitle(title)
}
data.frame(x=0:20, y=dpois(0:20, 5)) %>%
  plot.dist.disc("Distribución Poisson; lambda=5")
```
- Como el parámetro $\lambda$ indica el promedio, la pregunta puede formularse cómo ¿Existe un cambio de lambda?
- Podemos usar dos parámetros $\lambda_1$ y $\lambda_2$, para representar estos posibles dos estados
- Lo único que nos falta es un parámetro $\tau$, el cual representa el día en el cuál ocurre el cambio

---
## Ejemplo: Cambio de Media
- Hay que escoger distribuciones *a priori* para estos tres parámetros
- Suponiendo que no poseemos información previa, una buena idea es escoger la misma distribución para $\lambda_1$ y $\lambda_2$ y una no informativa para $\tau$. Por ejemplo:
```{r, echo=F, warning=F, message=F, fig.height=3.5, fig.align='center', fig.width=8}
library(gridExtra)
alpha <- 1/mean(counts$n)
grid.arrange(
  data.frame(x=0:2000/20, y=dexp(0:2000/20, r=alpha)) %>% 
    plot.dist.cont(expression(paste(lambda[1], ", ", lambda[2]))),
  data.frame(x=1:nrow(counts), y=1/nrow(counts)) %>% plot.dist.disc(expression(tau)) + 
    ylim(c(0, 0.02)),
  ncol=2
)
```

- Escogimos una distribución exponencial para $\lambda_1$, $\lambda_2$, con (hiper)parámetro $\alpha$ positivo
- La distribución *a priori* para $\tau$ es una discreta uniforme
---
## Ejemplo: Cambio de Media
#### Modelado en PyMC3 (Parámetros)

```{python, echo=T, eval=F}
import numpy as np
import pymc3 as pm

data = np.loadtxt("data/txtdata.csv")
n_obs = len(data)
day = np.arange(n_obs)

# define model
with pm.Model() as model:
    # hyper parameters
    alpha = 1/data.mean()
    
    # parameters
    tau = pm.DiscreteUniform("tau", lower=0, upper=n_obs)
    lambda1 = pm.Exponential("lambda1", alpha)
    lambda2 = pm.Exponential("lambda2", alpha)
    lambda_i = pm.math.switch(day < tau, lambda1, lambda2)
```

---
## Ejemplo: Cambio de Media
#### Modelado en PyMC3 (Observaciones y Predictiva)
```{python, echo=T, eval=F}
with model:
    # observations
    obs = pm.Poisson("obs", lambda_i, observed=data)
    
    # predictive distributions:
    pred1 = pm.Poisson("pred1", lambda1)
    pred2 = pm.Poisson("pred2", lambda2)
  
```

- Después de la inferencia, `pred1` y `pred2` indicaran la distribución predictiva cuando $\lambda =  \lambda_1$ y $\lambda = \lambda_2$, correspondientemente

#### Inferencia en PyMC3
```{python, echo=T, eval=F}
# perform inference
with model:
    step = pm.Metropolis()
    trace = pm.sample(10000, tune = 5000, step = step)
```
---

## Ejemplo: Cambio de Media
#### Preguntas específicas
- En la variable `trace` se encuentran las muestras tomadas de las distribuciones posteriores y de las predictivas.
- Esta muestra nos permite contestar una gran variedad de 'preguntas'
```{python, echo=T, eval=F}
# Expected L1
print(trace["lambda1"].mean()) # 17.78916798570392
# Expected L2
print(trace["lambda2"].mean()) # 22.669546949779832
# Probability L2 > L1:
print((trace["lambda2"] > trace["lambda1"]).mean()) # 0.9914
# Probability tau = 44
print((trace["tau"] == 44).mean()) # 0.4808
# Probability 20+ messages before tau day
print((trace["pred1"] > 20).mean()) # 0.2478
# Probability 20+ messages after tau day
print((trace["pred2"] > 20).mean()) # 0.6715
```

---
## Ejemplo: Cambio de Media
### Posteriores
```{r, echo=F, warning=F, fig.height=6, fig.width=12, fig.align='center'}
posteriors <- read.csv("data/posteriors.csv")
grid.arrange(
  posteriors %>%
    select(lambda1, lambda2) %>%
    gather %>%
    mutate(key = factor(key, labels=c("lambda[1]", "lambda[2]"))) %>%
    ggplot(aes(x=value)) + 
    geom_histogram(binwidth=0.3, color="white") +
    facet_wrap(~key, scales="free_y", labeller = label_parsed, nrow=2) + 
    theme_minimal() +
    ggtitle(expression(paste(lambda[i], "|", D))) +
    ylab("# samples") + xlab(""),
  ggplot(posteriors, aes(x=tau)) + 
    geom_histogram(binwidth=1, color="white") + 
    theme_minimal() + 
    ggtitle(expression(paste(tau, "|", D))) + 
    ylab("") + xlab(""),
  ncol=2
)
```

---
## Ejemplo: Cambio de Media
### Predictiva(s)
```{r, echo=F, warning=F, fig.height=6, fig.width=12, fig.align='center'}
  posteriors %>%
    select(pred1, pred2) %>%
    gather %>%
    mutate(key = factor(key, labels=c("lambda[1]", "lambda[2]"))) %>%
    ggplot(aes(x=value)) + 
    geom_histogram(binwidth=1, color="white") +
    facet_wrap(~key, scales="free_y", labeller = label_parsed, nrow=2) + 
    theme_minimal() +
    ggtitle(expression(paste(n, "|", lambda[i]))) +
    ylab("# samples") + xlab("")
```


---
## Conclusiones
La **Estadística Bayesiana** provee un *framework* de análisis de datos que propone soluciones para problemas prevalentes en otras técnicas como el *Machine Learning* y la Estadística frecuentista.

La **Programación Probabilística** resuelve algunos de los problemas prácticos relacionados con la inferencia Bayesiana, poniendo al alcance del Científico de Datos estas técnicas, sin necesidad de resolver problemas específicos relacionados a cada modelo.

Tomados juntos, proveen una manera práctica y poderosa de resolver preguntas acerca de incertidumbre y causalidad. En particular, ayudan a disminuir el problema de **sobrecertidumbre** en las predicciones, que puede tener consecuencias catastróficas según la aplicación.

---
class: middle, center, inverse
background-image: url(resources/DataDayMexico.png)
background-size: 256px
background-position: 93% 5%
## ¡Gracias!

twitter: [@arinarmo](https://twitter.com/arinarmo), github: [arinarmo](https://github.com/arinarmo)

### Referencias
[*Bayesian Reasoning and Machine Learning*](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf) - David Barber 

[*Bayesian Methods for Hackers*](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) - Cameron Davidson-Pilon

[*Probabilistic Programming*](http://blog.fastforwardlabs.com/2017/01/30/the-algorithms-behind-probabilistic-programming.html) - Fast Forward Labs

.footnote[ 
  [Repositorio con la plática](http://github.com/arinarmo/love_uncertainty): arinarmo/love_uncertainty 
]

